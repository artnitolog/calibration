{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Environment\n\n### Main imports\n\n# !pip install --upgrade \"numpy==1.20.2\"\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\n\n### Plots\n\nfrom matplotlib.cm import get_cmap\nimport matplotlib.pyplot as plt\nfrom IPython.display import set_matplotlib_formats\nfrom cycler import cycler\n\nplt.rc('axes', axisbelow=True, grid=True)\nplt.rc('grid', c='grey', ls=':')\nplt.rc('font', family='serif')\nplt.rc('axes', prop_cycle=cycler(color='bmrcgyk'))\nplt.rc('image', cmap='gist_rainbow')  # gist_rainbow\nplt.rc('savefig', bbox='tight', pad_inches=0.1, format='pdf')\n# set_matplotlib_formats('png')\n\n### Also\n\nrs = {'random_state': 0}\nrng = np.random.default_rng(seed=0)","metadata":{"id":"0HVjeoNjqw9A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics, visualization","metadata":{"id":"kq3Dre8nw3ng"}},{"cell_type":"code","source":"def bins_reliability_binary(y_true, y_confs, n_bins=10):\n    '''\n    Args:\n        y_true: np.array (n,) of 0 and 1, real classes\n        y_confs: np.array (n,), predicted probabilities of positive class\n        n_bins: int, number of bins\n    \n    Returns:\n        bin_confs: np.array (n,), mean confidence for each bin\n        bin_accs: np.array (n,), frequency of positives for each bin\n        weights: np.array (n,), normalized number of samples for each bin\n    '''\n    bins = np.linspace(0, 1, n_bins + 1)\n    # [0, 0.1), [0.1, 0.2), ..., [0.9, 1.0 + eps)\n    bins[-1] = 1 + 1e-10\n    # find which bin each sample is assigned to\n    bin_inds = np.digitize(y_confs, bins, right=False) - 1\n    # count number of samples in each bin\n    total = np.bincount(bin_inds, minlength=n_bins)\n    # find mean confidence for each bin\n    bin_confs = np.bincount(bin_inds, y_confs, minlength=n_bins)\n    np.divide(bin_confs, total, out=bin_confs, where=total!=0)\n    # find accuracy for each bin\n    bin_accs = np.bincount(bin_inds, y_true, minlength=n_bins)\n    np.divide(bin_accs, total, out=bin_accs, where=total!=0)\n    weights = total / total.sum()\n    return bin_confs, bin_accs, weights\n\ndef bins_reliability_multiclass(true_classes, confs, n_bins=10):\n    '''\n    Args:\n        true_classes: np.array (n,) of integers in range(0, n_classes)\n        confs: np.array (n, n_classes) of predicted probabilities\n        n_bins: int, number of bins\n    \n    Returns:\n        bin_confs: np.array (n,), mean confidence for each bin\n        bin_accs: np.array (n,), accuracy for each bin\n        weights: np.array (n,), normalized number of samples for each bin\n    '''\n    is_correct = (true_classes == confs.argmax(axis=1))\n    prediction_confs = confs.max(axis=1)\n    return bins_reliability_binary(is_correct, prediction_confs, n_bins)","metadata":{"id":"doAYhNiYMXuP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.special import softmax\nfrom sklearn.metrics import log_loss\n\n\ndef ECE(bin_confs, bin_accs, weights):\n    '''\n    Args (returns from bins_reliability):\n        bin_confs: np.array (n,), mean confidence for each bin\n        bin_accs: np.array (n,), accuracy for each bin\n        weights: np.array (n,)\n\n    Returns:\n        ece: expected calibration error\n    '''\n    diffs = np.abs(bin_confs - bin_accs)\n    ece = np.average(diffs, weights=weights)\n    return ece\n\ndef MCE(bin_confs, bin_accs, weights=None):\n    '''\n    Args (returns from bins_reliability):\n        bin_confs: np.array (n,), mean confidence for each bin\n        bin_accs: np.array (n,), accuracy for each bin\n        weights: np.array (n,), unused\n\n    Returns:\n        mce: maximum calibration error\n    '''\n    diffs = np.abs(bin_confs - bin_accs)\n    mce = diffs.max()\n    return mce\n\ndef BS(true_classes, confs):\n    onehot = np.zeros_like(confs)\n    onehot[np.arange(len(confs)), true_classes] = 1\n    return ((onehot - confs) ** 2).sum(axis=1).mean()\n\ndef all_metrics(true_classes, confs=None, n_bins=15, mul=100,\n                return_rel=False):\n    '''\n    Args:\n        true_classes: np.array (n,) of integers in range(0, n_classes)\n        confs: np.array (n, n_classes) of predicted probabilities\n        n_bins: int, number of bins (for computing binning metrics)\n        mul: float, multiplier for metrics with values in range [0, 1],\n             default is 100\n        return_rel: bool, whether to return (bin_confs, bin_accs, weights) - \n                    returns of bins_reliability_multiclass, default is False\n    \n    Returns:\n        dictionary with metrics:\n            'ACC': accuracy times mul,\n            'ECE': expected calibration error times mul,\n            'MCE': maximum calibration error times mul,\n            'NLL': negative log likelihood,\n            'BS': Brier score\n        rel: tuple (check Args)\n    '''\n    if confs is None:\n        confs = softmax(logits, axis=1)\n    metrics = {}\n    metrics['ACC'] = (confs.argmax(axis=1) == true_classes).mean() * mul\n    rel = bins_reliability_multiclass(true_classes, confs, n_bins)\n    metrics['ECE'] = ECE(*rel) * mul\n    metrics['MCE'] = MCE(*rel) * mul\n    metrics['BS'] = BS(true_classes, confs)\n    metrics['NLL'] = log_loss(true_classes, confs)\n    if return_rel:\n        return metrics, rel\n    else:\n        return metrics","metadata":{"id":"GClxCKSlw90A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _hist_plot_adjust(pad=0.00):\n    plt.xlim(-pad, 1 + pad)\n    plt.ylim(-pad, 1 + pad)\n    plt.xticks(np.linspace(0, 1, 6))\n    plt.yticks(np.linspace(0.2, 1.0, 5))\n    plt.gca().set_aspect('equal')\n    plt.gca().tick_params(length=0)\n\ndef _reliability_plot(bin_confs, bin_accs, weights=None,\n                      name='reliability plot', acc_label='accuracy',\n                      show=False, path=None):\n    '''\n    Args:\n        bin_confs: np.array (n,), mean confidence for each bin\n        bin_accs: np.array (n,), accuracy for each bin or class frequencies\n        weights: np.array (n,)\n        name: str, plot title\n        acc_label: str, meaning of bin_accs\n        show: bool, if True, plt.show() will be called\n        path: str, location to save figure, default is None\n\n    '''\n    n_bins = len(bin_confs)\n    bins = np.linspace(0, 1, n_bins + 1)\n    centers = (bins[:-1] + bins[1:]) / 2\n    plt.bar(centers, bin_confs, color=(1, 0, 0, 0.5), edgecolor='black',\n            label='confidence', width=1/n_bins)\n    plt.bar(centers, bin_accs, color=(0, 0, 1, 0.5), edgecolor='black',\n            label=acc_label, width=1/n_bins)\n    if weights is not None:\n        plt.bar(centers, weights, color=(0, 1.0, 0.5, 0.8), edgecolor='black',\n                label='weight', width=0.5/n_bins)\n    plt.plot([0, 1], [0, 1], color='silver', linestyle='--')\n    plt.xlabel('confidence')\n    #  plt.ylabel('accuracy') not only...\n    plt.legend()\n    plt.title(name)\n    _hist_plot_adjust()\n    if show:\n        plt.show()\n    if path is not None:\n        plt.savefig(path)\n\ndef reliability_plot(confs, true_classes, n_bins=10, **kwargs):\n    '''\n    Args:\n        confs: np.array (n, n_classes) of predicted probabilities\n        true_classes: np.array (n,) of integers in range(0, n_classes)\n        n_bins: int, number of bins (for computing binning metrics)\n        kwargs: keyword arguments passed to _reliability_plot\n\n    '''\n    rel = bins_reliability_multiclass(true_classes, confs, n_bins)\n    _reliability_plot(*rel, **kwargs) ","metadata":{"id":"orz7JgWSw-8U","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Методы калибровки","metadata":{"id":"zgtChy625oyr"}},{"cell_type":"markdown","source":"## Transform confidences\ninput: confidence,\noutput: calibrated confidence","metadata":{}},{"cell_type":"code","source":"from functools import partial\nfrom sklearn.isotonic import IsotonicRegression\n\nclass HistogramBinningBinary:\n    def __init__(self, n_bins=10):\n        self.n_bins = n_bins\n        self.thetas = None\n        self.bins = np.linspace(0, 1, n_bins + 1)\n        self.bins[-1] += 1e-10\n\n    def fit(self, y_confs, y_true):\n        '''\n        Args:\n            y_confs: np.array estimated probabilities of positive class\n            y_true: np.array of 0 and 1\n        '''\n        _, thetas, weights = bins_reliability_binary(y_true, y_confs,\n                                                     n_bins=self.n_bins)\n        centers = ((self.bins[:-1] + self.bins[1:]) * 0.5)\n        thetas[weights == 0] = centers[weights == 0]\n        self.thetas = thetas\n    \n    def transform(self, y_confs):\n        '''\n        Args:\n            y_confs: uncalibrated estimated probability of positive class\n        Returns:\n            y_confs_calib: calibrated probabilities\n        '''\n        y_confs_calib = self.thetas[np.digitize(y_confs, self.bins) - 1]\n        return y_confs_calib\n\nclass IRBinary(IsotonicRegression):\n    '''\n    Isotonic regression wrapper for binary calibration.\n    '''\n    def __init__(self):\n        super().__init__(increasing=True, out_of_bounds='clip',\n                         y_min=0.0, y_max=1.0)\n\nclass CalibratorOvR:\n    def __init__(self, base, **kwargs):\n        '''\n        Args:\n            base: class of binary calibrator\n            kwargs: keyword arguments to initialize each calibrator\n        '''\n        self.base = partial(base, **kwargs)\n        self.ovr_calibrators = []\n    \n    def fit(self, confs, true_classes):\n        '''\n        Args:\n            confs: np.array (n, n_classes) of predicted probabilities (uncalibrated)\n            true_classes: np.array (n,) of integers in range(0, n_classes)\n        '''\n        self.ovr_calibrators = []\n        for class_ in range(confs.shape[1]):\n            calibrator = self.base()\n            calibrator.fit(confs[:, class_], (true_classes == class_).astype(int))\n            self.ovr_calibrators.append(calibrator)\n    \n    def transform(self, confs):\n        '''\n        Args:\n            confs: np.array (n, n_classes) of predicted probabilities\n        Returns:\n            calbrated_confs: np.array (n, n_classes) of calibrated probabilities\n        '''\n        cal_confs = np.stack([calibrator.transform(confs[:, class_]) for\n            class_, calibrator in enumerate(self.ovr_calibrators)], axis=1)\n        cal_confs /= cal_confs.sum(axis=1, keepdims=True)\n        return cal_confs\n\nclass HistogramBinningMulticlass(CalibratorOvR):\n    def __init__(self, n_bins=15):\n        super().__init__(HistogramBinningBinary, n_bins=n_bins)\n\nclass IsotonicRegressionMulticlass(CalibratorOvR):\n    def __init__(self):\n        super().__init__(IRBinary)","metadata":{"id":"Q_z-LkUg9YZZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transform logits\ninput: logits,\noutput: calibrated logits","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import cross_entropy\n\ndef tt(np_array):\n    return torch.from_numpy(np_array)\n\nclass LogitScaling:\n    def __init__(self, scale_type='temperature', bias_type='none'):\n        '''\n        scale_type: str:\n            'temperature': single-parameter logits' scaling\n            'vector': vector scaling (logits multiplied by diagonal matrix)\n            'matrix': matrix scaling\n        bias_type: str\n            'intercept': one bias (number) for all classes\n            'vector': individual biases for each class\n            'none': no bias term\n        '''\n        self.scale_type = scale_type\n        self.bias_type = bias_type\n        self.scale = None\n        self.bias = None\n    \n    def fit(self, logits_val, targets_val, device='cpu', lr=1.0, max_iter=1000):\n        logits_cal = tt(logits_val).to(device)\n        targets_cal = tt(targets_val).to(device)\n        kwargs = {'dtype': logits_cal.dtype, 'requires_grad': True, 'device': device}\n        if self.scale_type == 'temperature':\n            scale = torch.tensor(1.0, **kwargs)\n        elif self.scale_type == 'vector':\n            scale = torch.ones(logits_cal.shape[1], **kwargs)\n        elif scale_type == 'matrix':\n            scale = torch.eye(logits_cal.shape[1], **kwargs)\n        params = [scale]\n        if self.bias_type == 'intercept':\n            bias = torch.tensor(0.0, **kwargs)\n            params.append(bias)\n        elif self.bias_type == 'vector':\n            bias = torch.zeros(logits_cal.shape[1], **kwargs)\n            params.append(bias)\n        else:\n            bias = torch.tensor(0, requires_grad=False, dtype=logits_cal.dtype, device=device)\n            \n        optimizer = torch.optim.LBFGS(params, lr=lr, max_iter=max_iter)\n        def closure():\n            optimizer.zero_grad()\n            if self.scale_type == 'matrix':\n                loss = cross_entropy(logits_cal @ scale + bias, targets_cal)\n            else:\n                loss = cross_entropy(logits_cal * scale + bias, targets_cal)\n            loss.backward()\n            return loss\n        optimizer.step(closure)\n        self.n_iter = optimizer.state[scale]['n_iter']\n        self.scale = scale.detach().cpu().numpy()\n        self.bias = bias.detach().cpu().numpy()\n    \n    def transform(self, logits_test):\n        if self.scale_type == 'matrix':\n            return softmax(logits_test @ self.scale + self.bias, axis=1)\n        else:\n            return softmax(logits_test * self.scale + self.bias, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute results","metadata":{}},{"cell_type":"code","source":"def get_logits(path):\n    logitss = {}\n    targets = None\n    for fname in os.listdir(path):\n        if fname == 'targets.txt':\n            with open(os.path.join(path, fname), 'r') as fin:\n                targets = np.array([int(target) for target in fin.read().split()])\n        elif fname[-3:] == '.pt':\n            logitss[fname[:-3]] = torch.load(os.path.join(path, fname)).numpy()\n    return logitss, targets    \n\ndef upd_metrics(metrics_dict, new_metrics,\n                calib_name, model_dataset_tuple):\n    for metric_name, value in new_metrics.items():\n        metrics_dict[metric_name][calib_name][model_dataset_tuple] = value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import defaultdict\n\nlogit_path = '../input/calibration/logits/'\ndataset_names = ['cifar10_v1',\n                 'cifar100',\n                 'imagenet']\n\ncalibrators_confs = {\n    'Hist-binning': HistogramBinningMulticlass(n_bins=15),\n    'Isotonic': IsotonicRegressionMulticlass(),\n}\n\ncalibrators_logits = {\n    'T-scaling': LogitScaling(scale_type='temperature', bias_type='none'),\n    'V-scaling': LogitScaling(scale_type='vector', bias_type='none'),\n    'V-scaling + bias': LogitScaling(scale_type='vector', bias_type='vector'),\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_val = defaultdict(lambda: defaultdict(dict))\nmetrics_test = defaultdict(lambda: defaultdict(dict))\n\nfor dataset_name in dataset_names:\n    print(f'== dataset {dataset_name} ==')\n    fpath = os.path.join(logit_path, dataset_name)\n    logitss, targets = get_logits(fpath)\n    if dataset_name == 'cifar10_v1':\n        dataset_name = 'cifar10'\n    for model_name, logits in logitss.items():\n        print(f'{model_name}, ', end='')\n        model_dataset_tuple = (dataset_name, model_name)\n        logits_val, logits_test, targets_val, targets_test = train_test_split(\n        logits, targets, test_size=0.5, stratify=targets, **rs)\n        confs_val = softmax(logits_val, axis=1)\n        confs_test = softmax(logits_test, axis=1)\n        \n        # No calibration\n        upd_metrics(metrics_val, all_metrics(targets_val, confs_val),\n                    'До калибровки', model_dataset_tuple)\n        upd_metrics(metrics_test, all_metrics(targets_test, confs_test),\n                    'До калибровки', model_dataset_tuple)\n        \n        # Transforming confs\n        for cal_name, calibrator in calibrators_confs.items():\n            calibrator.fit(confs_val, targets_val)\n            confs_val_cal = calibrator.transform(confs_val)\n            confs_test_cal = calibrator.transform(confs_test)\n            upd_metrics(metrics_val, all_metrics(targets_val, confs_val_cal),\n                        cal_name, model_dataset_tuple)\n            upd_metrics(metrics_test, all_metrics(targets_test, confs_test_cal),\n                        cal_name, model_dataset_tuple)\n        \n        # Transforming logits\n        for cal_name, calibrator in calibrators_logits.items():\n            calibrator.fit(logits_val, targets_val)\n            confs_val_cal = calibrator.transform(logits_val)\n            confs_test_cal = calibrator.transform(logits_test)\n            upd_metrics(metrics_val, all_metrics(targets_val, confs_val_cal),\n                        cal_name, model_dataset_tuple)\n            upd_metrics(metrics_test, all_metrics(targets_test, confs_test_cal),\n                        cal_name, model_dataset_tuple)\n        \n    print('DONE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_tex(data, mode='min', format_string='%.2f'):\n    if mode == 'min':\n        mask = data != data.min()\n    elif mode == 'max':\n        mask = data != data.max()\n    else:\n        mask = np.ones_like(data, dtype=bool)\n    bolded = data.apply(lambda x : 'BOLDLEFT%sBOLDRIGHT' % format_string % x)\n    formatted = data.apply(lambda x : format_string % x)\n    return formatted.where(mask, bolded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"format_kwargs = {\n    'ACC': {'mode': 'max', 'format_string': '%.3f'},\n    'ECE': {'mode': 'min', 'format_string': '%.2f'},\n    'MCE': {'mode': 'min', 'format_string': '%.2f'},\n    'BS': {'mode': 'min', 'format_string': '%.3f'},\n    'NLL': {'mode': 'min', 'format_string': '%.3f'},\n}\n\napp = 'Значения метрики приводятся для тестовой выборки до и после калибровки.'\ncaptions = {\n    'ACC': r'Accuracy, \\% -- доля правильных ответов (больше -- лучше). ' + app,\n    'ECE': r'ECE, \\% -- Expected Calibration Error, 15 бинов (меньше -- лучше). ' + app,\n    'MCE': r'MCE, \\% -- Maximum Calibration Error, 15 бинов (меньше -- лучше). ' + app,\n    'BS': r'Brier Score (меньше -- лучше). ' + app,\n    'NLL': r'Negative Log-Likelihood (меньше -- лучше). ' + app,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir tabs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for metric_name in metrics_test:\n    df = pd.DataFrame(metrics_test[metric_name])\n    df = df.sort_index().apply(format_tex, **format_kwargs[metric_name], axis=1)\n    df = df.reset_index().rename({'level_0': 'Данные', 'level_1': 'Модель'}, axis=1)\n    df_latex = df.to_latex(label=f'tab:metrics:{metric_name}', index=False, position='h!', caption=captions[metric_name])\n    # bold extreme values\n    df_latex = df_latex.replace('BOLDLEFT', r'\\textbf{').replace('BOLDRIGHT', r'}')\n    # Adjust table to text width\n    df_latex = df_latex.replace(r'\\begin{tabular}', r'\\resizebox{\\textwidth}{!}{\\begin{tabular}')\n    df_latex = df_latex.replace(r'\\end{tabular}', r'\\end{tabular}}')\n    # Center values\n    df_latex = df_latex.replace(r'{llllllll}', r'{llcccccc}')\n    with open(f'tabs/metrics_{metric_name}.tex', 'w') as fout:\n        fout.write(df_latex)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r tabs.zip tabs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Визуализации","metadata":{"id":"lVE2IfbliDwD"}},{"cell_type":"code","source":"!mkdir vis","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Модельные","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(n_samples=20000, n_features=19,\n                           n_informative=3, n_redundant=10,\n                           random_state=0)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train, test_size=0.3, random_state=0)\nmodel = LinearSVC(random_state=0, max_iter=10000)\n_ = model.fit(X_train, y_train)\n\ny_val_confs = model.decision_function(X_val)\ny_min = y_val_confs.min()\ny_max = y_val_confs.max()\n\ny_val_confs = (y_val_confs - y_min) / (y_max - y_min)\ny_test_confs = model.decision_function(X_test)\ny_test_confs = (y_test_confs - y_min) / (y_max - y_min)\ny_test_confs = np.clip(y_test_confs, 0, 1)","metadata":{"id":"3l5ZNycMRiDu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def binconf2mc(y_confs):\n    return np.stack([1 - y_confs, y_confs], axis=1)\n\nbin_confs, bin_accs, weights = bins_reliability_binary(y_test, y_test_confs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(11, 5))\nplt.subplot(131)\nplt.plot(bin_confs, bin_accs, marker='o', label='calibration curve', color='m')\nplt.plot([0, 1], [0, 1], color='silver', linestyle='--', label='perfect calibraion')\n_hist_plot_adjust(pad=0.0)\nplt.xlabel('confidence')\nplt.ylabel('positive frequency')\nplt.title('Reliability plot (binary)')\nplt.legend()\nplt.text(0.5, -0.2, '(a)', ha='center')\n\nplt.subplot(132)\n_reliability_plot(bin_confs, bin_accs, weights, name='Reliability diagram (binary)', acc_label='positive frequency')\nplt.text(0.5, -0.2, '(b)', ha='center')\n\nplt.subplot(133)\nreliability_plot(binconf2mc(y_test_confs), y_test, name='Reliability diagram (multiclass)')\nplt.text(0.5, -0.2, '(c)', ha='center')\n\nplt.tight_layout()\nplt.savefig('vis/rel_intro')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Нейросети","metadata":{}},{"cell_type":"markdown","source":"### <font color=blue>WRAP IT TO THE GENERAL FUNCTION</font>","metadata":{}},{"cell_type":"code","source":"metrics_val = defaultdict(lambda: defaultdict(dict))\nmetrics_test = defaultdict(lambda: defaultdict(dict))\n\nplt.figure(figsize=(11, 8))\ndataset_name = 'cifar100'  # !\nfpath = os.path.join(logit_path, dataset_name)\nlogitss, targets = get_logits(fpath)\nmodel_name = 'shufflenetv2_x0_5'  # !\nlogits = logitss[model_name]\n\nmodel_dataset_tuple = (dataset_name, model_name)\nlogits_val, logits_test, targets_val, targets_test = train_test_split(\nlogits, targets, test_size=0.5, stratify=targets, **rs)\nconfs_val = softmax(logits_val, axis=1)\nconfs_test = softmax(logits_test, axis=1)\n\n# No calibration\nupd_metrics(metrics_val, all_metrics(targets_val, confs_val),\n            'До калибровки', model_dataset_tuple)\nupd_metrics(metrics_test, all_metrics(targets_test, confs_test),\n            'До калибровки', model_dataset_tuple)\n\ni = 1\nplt.subplot(2, 3, i)\nreliability_plot(confs_test, targets_test, name='До калибровки')\n\n# Transforming confs\nfor cal_name, calibrator in calibrators_confs.items():\n    calibrator.fit(confs_val, targets_val)\n    confs_val_cal = calibrator.transform(confs_val)\n    confs_test_cal = calibrator.transform(confs_test)\n    upd_metrics(metrics_val, all_metrics(targets_val, confs_val_cal),\n                cal_name, model_dataset_tuple)\n    upd_metrics(metrics_test, all_metrics(targets_test, confs_test_cal),\n                cal_name, model_dataset_tuple)\n    i += 1\n    plt.subplot(2, 3, i)\n    reliability_plot(confs_test_cal, targets_test, name=cal_name)\n\n# Transforming logits\nfor cal_name, calibrator in calibrators_logits.items():\n    calibrator.fit(logits_val, targets_val)\n    confs_val_cal = calibrator.transform(logits_val)\n    confs_test_cal = calibrator.transform(logits_test)\n    upd_metrics(metrics_val, all_metrics(targets_val, confs_val_cal),\n                cal_name, model_dataset_tuple)\n    upd_metrics(metrics_test, all_metrics(targets_test, confs_test_cal),\n                cal_name, model_dataset_tuple)\n    i += 1\n    plt.subplot(2, 3, i)\n    reliability_plot(confs_test_cal, targets_test, name=cal_name)\n    \nplt.tight_layout()\nplt.savefig(f'calibration_{dataset_name}_{model_name}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_val = defaultdict(lambda: defaultdict(dict))\nmetrics_test = defaultdict(lambda: defaultdict(dict))\n\nplt.figure(figsize=(11, 8))\ndataset_name = 'imagenet'  # !\nfpath = os.path.join(logit_path, dataset_name)\nlogitss, targets = get_logits(fpath)\nmodel_name = 'tf_efficientnet_b8'  # !\nlogits = logitss[model_name]\n\nmodel_dataset_tuple = (dataset_name, model_name)\nlogits_val, logits_test, targets_val, targets_test = train_test_split(\nlogits, targets, test_size=0.5, stratify=targets, **rs)\nconfs_val = softmax(logits_val, axis=1)\nconfs_test = softmax(logits_test, axis=1)\n\n# No calibration\nupd_metrics(metrics_val, all_metrics(targets_val, confs_val),\n            'До калибровки', model_dataset_tuple)\nupd_metrics(metrics_test, all_metrics(targets_test, confs_test),\n            'До калибровки', model_dataset_tuple)\n\ni = 1\nplt.subplot(2, 3, i)\nreliability_plot(confs_test, targets_test, name='До калибровки')\n\n# Transforming confs\nfor cal_name, calibrator in calibrators_confs.items():\n    calibrator.fit(confs_val, targets_val)\n    confs_val_cal = calibrator.transform(confs_val)\n    confs_test_cal = calibrator.transform(confs_test)\n    upd_metrics(metrics_val, all_metrics(targets_val, confs_val_cal),\n                cal_name, model_dataset_tuple)\n    upd_metrics(metrics_test, all_metrics(targets_test, confs_test_cal),\n                cal_name, model_dataset_tuple)\n    i += 1\n    plt.subplot(2, 3, i)\n    reliability_plot(confs_test_cal, targets_test, name=cal_name)\n\n# Transforming logits\nfor cal_name, calibrator in calibrators_logits.items():\n    calibrator.fit(logits_val, targets_val)\n    confs_val_cal = calibrator.transform(logits_val)\n    confs_test_cal = calibrator.transform(logits_test)\n    upd_metrics(metrics_val, all_metrics(targets_val, confs_val_cal),\n                cal_name, model_dataset_tuple)\n    upd_metrics(metrics_test, all_metrics(targets_test, confs_test_cal),\n                cal_name, model_dataset_tuple)\n    i += 1\n    plt.subplot(2, 3, i)\n    reliability_plot(confs_test_cal, targets_test, name=cal_name)\n    \nplt.tight_layout()\nplt.savefig(f'calibration_{dataset_name}_{model_name}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_val = defaultdict(lambda: defaultdict(dict))\nmetrics_test = defaultdict(lambda: defaultdict(dict))\n\nplt.figure(figsize=(11, 8))\ndataset_name = 'cifar10_v1'  # !\nfpath = os.path.join(logit_path, dataset_name)\nlogitss, targets = get_logits(fpath)\nmodel_name = 'googlenet'  # !\nlogits = logitss[model_name]\n\nmodel_dataset_tuple = (dataset_name, model_name)\nlogits_val, logits_test, targets_val, targets_test = train_test_split(\nlogits, targets, test_size=0.5, stratify=targets, **rs)\nconfs_val = softmax(logits_val, axis=1)\nconfs_test = softmax(logits_test, axis=1)\n\n# No calibration\nupd_metrics(metrics_val, all_metrics(targets_val, confs_val),\n            'До калибровки', model_dataset_tuple)\nupd_metrics(metrics_test, all_metrics(targets_test, confs_test),\n            'До калибровки', model_dataset_tuple)\n\ni = 1\nplt.subplot(2, 3, i)\nreliability_plot(confs_test, targets_test, name='До калибровки')\n\n# Transforming confs\nfor cal_name, calibrator in calibrators_confs.items():\n    calibrator.fit(confs_val, targets_val)\n    confs_val_cal = calibrator.transform(confs_val)\n    confs_test_cal = calibrator.transform(confs_test)\n    upd_metrics(metrics_val, all_metrics(targets_val, confs_val_cal),\n                cal_name, model_dataset_tuple)\n    upd_metrics(metrics_test, all_metrics(targets_test, confs_test_cal),\n                cal_name, model_dataset_tuple)\n    i += 1\n    plt.subplot(2, 3, i)\n    reliability_plot(confs_test_cal, targets_test, name=cal_name)\n\n# Transforming logits\nfor cal_name, calibrator in calibrators_logits.items():\n    calibrator.fit(logits_val, targets_val)\n    confs_val_cal = calibrator.transform(logits_val)\n    confs_test_cal = calibrator.transform(logits_test)\n    upd_metrics(metrics_val, all_metrics(targets_val, confs_val_cal),\n                cal_name, model_dataset_tuple)\n    upd_metrics(metrics_test, all_metrics(targets_test, confs_test_cal),\n                cal_name, model_dataset_tuple)\n    i += 1\n    plt.subplot(2, 3, i)\n    reliability_plot(confs_test_cal, targets_test, name=cal_name)\n    \nplt.tight_layout()\nplt.savefig(f'calibration_cifar10_{model_name}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r vis.zip vis","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OvR-checks","metadata":{}},{"cell_type":"markdown","source":"### No calibration","metadata":{"id":"0WpvYxtUAnVu"}},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_test, y_test_confs, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_test, binconf2mc(y_test_confs), n_bins=10)\n_reliability_plot(*rel)","metadata":{"id":"6NEtH82ZWnUE","outputId":"90c1591b-ca3f-4317-f372-48c3d746d07a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_val, y_val_confs, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_val, binconf2mc(y_val_confs), n_bins=10)\n_reliability_plot(*rel)","metadata":{"id":"P9jnkNpCixiX","outputId":"8dd75b5b-08e8-4bff-9bcc-783f16168afa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histogram binning (binary)","metadata":{"id":"y3GPtVilArGO"}},{"cell_type":"code","source":"calibrator = HistogramBinningBinary(n_bins=10)\ncalibrator.fit(y_val_confs, y_val)\ny_val_confs_calib = calibrator.transform(y_val_confs)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_val, y_val_confs_calib, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_val, binconf2mc(y_val_confs_calib), n_bins=10)\n_reliability_plot(*rel, show=False)","metadata":{"id":"uSN4QoghAfT1","outputId":"d6a32310-1c90-4a4c-b437-8dcabb1b4693"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calibrator = HistogramBinningBinary(n_bins=10)\ncalibrator.fit(y_val_confs, y_val)\ny_test_confs_calib = calibrator.transform(y_test_confs)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_test, y_test_confs_calib, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_test, binconf2mc(y_test_confs_calib), n_bins=10)\n_reliability_plot(*rel)","metadata":{"id":"7t7TZ6NvlpGp","outputId":"05ceeb5f-0343-445f-9bcb-47bc10fa10b8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Isotonic Regression (binary)","metadata":{"id":"e8rggEuoDTiQ"}},{"cell_type":"code","source":"calibrator = IRBinary()\ncalibrator.fit(y_val_confs, y_val)\ny_val_confs_calib = calibrator.transform(y_val_confs)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_val, y_val_confs_calib, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_val, binconf2mc(y_val_confs_calib), n_bins=10)\n_reliability_plot(*rel, show=False)","metadata":{"id":"TDuUFzZJAaBN","outputId":"93b704e7-0d89-42c8-9fb1-facd9740b613"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calibrator = IRBinary()\ncalibrator.fit(y_val_confs, y_val)\ny_test_confs_calib = calibrator.transform(y_test_confs)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_test, y_test_confs_calib, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_test, binconf2mc(y_test_confs_calib), n_bins=10)\n_reliability_plot(*rel, show=False)","metadata":{"id":"tAG6Sxe8w-55","outputId":"71fc666f-38ff-41d5-f6df-975f262550f8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histogram binning (multiclass)","metadata":{"id":"BWeDhYaDFS7R"}},{"cell_type":"code","source":"calibrator_mc = HistogramBinningMulticlass(n_bins=10)\ncalibrator_mc.fit(binconf2mc(y_val_confs), y_val)\ny_val_confs_calib_mc = calibrator_mc.transform(binconf2mc(y_val_confs))\ny_val_confs_calib = y_val_confs_calib_mc[:, 1]\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_val, y_val_confs_calib, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_val, y_val_confs_calib_mc, n_bins=10)\n_reliability_plot(*rel, show=False)","metadata":{"id":"gDFz-Dn99Dme","outputId":"863a45a3-b27b-43ac-dbf0-b0245378d76d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calibrator_mc = HistogramBinningMulticlass(n_bins=10)\ncalibrator_mc.fit(binconf2mc(y_val_confs), y_val)\ny_test_confs_calib_mc = calibrator_mc.transform(binconf2mc(y_test_confs))\ny_test_confs_calib = y_test_confs_calib_mc[:, 1]\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_test, y_test_confs_calib, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_test, y_test_confs_calib_mc, n_bins=10)\n_reliability_plot(*rel, show=False)","metadata":{"id":"nfAabe4kG-0j","outputId":"a7fcda13-59b2-4125-dd9f-25fd6ce24563"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Isotonic regression (multiclass)","metadata":{"id":"WkI93KgOIuaQ"}},{"cell_type":"code","source":"calibrator_mc = IsotonicRegressionMulticlass()\ncalibrator_mc.fit(binconf2mc(y_val_confs), y_val)\ny_val_confs_calib_mc = calibrator_mc.transform(binconf2mc(y_val_confs))\ny_val_confs_calib = y_val_confs_calib_mc[:, 1]\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_val, y_val_confs_calib, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_val, y_val_confs_calib_mc, n_bins=10)\n_reliability_plot(*rel, show=False)","metadata":{"id":"iOi4DubUI4fo","outputId":"fd2ba0d0-1487-4e45-909c-cee6d706acb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calibrator_mc = IsotonicRegressionMulticlass()\ncalibrator_mc.fit(binconf2mc(y_val_confs), y_val)\ny_test_confs_calib_mc = calibrator_mc.transform(binconf2mc(y_test_confs))\ny_test_confs_calib = y_test_confs_calib_mc[:, 1]\n\nplt.figure(figsize=(10, 5))\nplt.subplot(121)\nrel = bins_reliability_binary(y_test, y_test_confs_calib, n_bins=10)\n_reliability_plot(*rel, acc_label='class 1 frequency', show=False)\nplt.subplot(122)\nrel = bins_reliability_multiclass(y_test, y_test_confs_calib_mc, n_bins=10)\n_reliability_plot(*rel, show=False)","metadata":{"id":"ryseNOoII-tJ","outputId":"9e8e1433-765d-4450-ea7f-e68bc9e4986b"},"execution_count":null,"outputs":[]}]}